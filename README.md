# SoC 2020 Unstructured
> **Mentor - Gagan Jain**

Note - Dates in certificate may not match the week in which it was completed because there were generally 1-2 problems which I left for sometime else and then had to eventually complete them for certificate.

## Progress Report - 

### Week 1 (23rd March - 29 March)

* Reading abou basic types of ML , Supervised Learning and Unsupervised learning. Notes are [HERE](https://github.com/MananKGarg/SoC_2020_Conversational_Chatbot/blob/master/Intro%20to%20ML)
* Gradient Descent Algorithm. [Notes](https://github.com/MananKGarg/SoC_2020_Conversational_Chatbot/tree/master/Week%201/Day%201)
* [Linear Regression](https://github.com/MananKGarg/SoC_2020_Conversational_Chatbot/tree/master/Day%201)
* [Coding Linear Regression](https://github.com/MananKGarg/SoC_2020_Conversational_Chatbot/tree/master/Day%203)
* Learning [Logistic Regression](https://github.com/MananKGarg/SoC_2020_Conversational_Chatbot/blob/master/Day%203/logistic%20regression%20for%20classification%20problems) for classification problem

### Week 2 (30 March - 5 April)

* Brushed up python once by completing Kaggle course. Click for [NOTES](https://github.com/MananKGarg/Python-Codes/tree/master/Notes) made and [CERTIFICATE](https://github.com/MananKGarg/Python-Codes/blob/master/Notes/Certificate.md)
* Completed Intro to Machine Learning course on Kaggle. Click for [NOTES](https://github.com/MananKGarg/Python-Codes/tree/master/ML%20Kaggle) and [CERTIFICATE](https://github.com/MananKGarg/Python-Codes/blob/master/ML%20Kaggle/7.%20Certificate.md)
* Learned various basics of ML like fitting and predicting data, Model Validation, Underfitting and Overfitting using Decision Tree model.
* Learned about Random Forest Model and it's advantages over Decision Tree

### Week 3 (6th April - 12th April)

* Started with the Deep Learning Neural Networks course on Coursera. 
* Started learning about Pandas and it's usage in data manipulation.
* Completed Pandas course on Kaggle. Click to view [NOTES](https://github.com/MananKGarg/Kaggle/tree/master/Kaggle%20Pandas) and [CERTIFICATE](https://github.com/MananKGarg/Kaggle/blob/master/Kaggle%20Pandas/7.%20Certificate.md)
* Learned about various methods and strategied involved in sorting, renaming, combining and Grouping data for better analysis of data using Pandas
* completed First and Second course on Coursera.

### Week 4 (13th April - 20th April)


* Started with learning more about convolutions and building computer vision models using Tensorflow and Keras.
* Moved on to Transfer Learning, Data Augmentation and learned how Stochastic Gradient Descent and Backpropagation can be used to better train the models.
* Learnt about Dropuots and Strides for working on bigger datasets.
* Completed the Kaggle course on Deep Learning. Click to view [NOTES](https://github.com/MananKGarg/Kaggle/tree/master/Kaggle%20Deep%20Learning) and [CERTIFICATE](https://user-images.githubusercontent.com/62146744/79679609-dad44780-8224-11ea-8b69-ad5e00739ee3.png)
* Made a model for digit recognising using MNIST dataset. Code is uploaded in this repo.
* Learnt about sources of error and ways to fine tune the parameters in machine learning models
* Preprocessed data on Kaggle to account for Missing Values and Categorical Variables firstly by coding it all seperately and then by using Pipelines
* Used XGBoost on Kaggle House Pricing Dataset and learnt about various kinds of Data Leakages.
* Completed the Kaggle Course on Intermediate Machine Learning. Not made Notes for this one yet but here's the [CERTIFICATE](https://github.com/MananKGarg/Kaggle/tree/master/Kaggle%20Intermediate%20Machine%20Learning)

